{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0de98f24d936b54738f034aaefff25bd15934cd624d7ab7c984fdb966cd675040",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "de98f24d936b54738f034aaefff25bd15934cd624d7ab7c984fdb966cd675040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [(1, 'deep', 'JJ'), (2, 'learning', 'NN'), (3, 'methods', 'NNS'), (5, 'many', 'JJ'), (7, 'layers', 'NNS'), (11, 'stratified', 'JJ'), (12, 'representation', 'NN'), (14, 'data', 'NNS'), (18, 'state-of-art', 'JJ'), (19, 'results', 'NNS'), (21, 'several', 'JJ'), (22, 'domains', 'NNS'), (26, 'deep', 'JJ'), (27, 'learning', 'NN'), (28, 'model', 'NN'), (29, 'designs', 'NNS'), (31, 'architectures', 'NNS'), (36, 'context', 'NN'), (38, 'natural', 'JJ'), (39, 'language', 'NN'), (40, 'processing', 'NN'), (42, 'nlp', 'JJ'), (46, 'survey', 'NN'), (49, 'brief', 'JJ'), (50, 'description', 'NN'), (53, 'advances', 'NNS'), (59, 'area', 'NN'), (61, 'deep', 'JJ'), (62, 'generative', 'JJ'), (63, 'modeling', 'NN'), (66, 'work', 'NN'), (68, 'most', 'JJS'), (71, 'papers', 'NNS'), (74, 'onwards', 'NNS'), (78, 'paper', 'NN'), (82, 'many', 'JJ'), (83, 'deep', 'JJ'), (84, 'learning', 'NN'), (85, 'models', 'NNS'), (92, 'generation', 'NN'), (94, 'text', 'NN'), (100, 'various', 'JJ'), (101, 'models', 'NNS'), (107, 'detailed', 'JJ'), (108, 'understanding', 'NN'), (110, 'past', 'JJ'), (112, 'present', 'JJ'), (115, 'future', 'NN'), (117, 'text', 'JJ'), \n",
    "(118, 'generation', 'NN'), (119, 'models', 'NNS'), (121, 'deep', 'JJ'), (122, 'learning', 'NN'), (126, 'dl', 'JJ'), (127, 'approaches', 'NNS'), (135, 'different', 'JJ'), (136, 'application', 'NN'), (137, 'domains', 'NNS'), (139, 'nlp', 'NN'), (144, 'survey', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN_GROUP = ['NN', 'NNS', 'NNP', 'NNPS'] # 4\n",
    "PRONOUN_GROUP = ['PRP', 'PRP$', 'WP', 'WP$'] # 4\n",
    "VERB_GROUP = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] # 6\n",
    "ADJECTIVE_GROUP = ['JJ', 'JJR', 'JJS'] # 3\n",
    "ADVERB_GROUP = ['RB', 'RBR', 'RBS', 'WRB']\n",
    "PREPOSITION_GROUP = ['IN']\n",
    "CONJUNCTION_GROUP = ['CC', 'IN']\n",
    "INTERJECTION_GROUP = ['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalGraph(object):\n",
    "    def __init__(self, filtered_tokens, N):\n",
    "        self.total_cnt = len(filtered_tokens)\n",
    "        words = [t[1] for t in filtered_tokens]\n",
    "        unique_words = list(Counter(words))\n",
    "        self.unique_cnt = len(unique_words)\n",
    "        self.T = self.unique_cnt // 3\n",
    "        self.conversion = {unique_words[i] : i for i in range(len(unique_words))}\n",
    "        self.V = {self.conversion[v] : 1 for v in unique_words}\n",
    "        self.E = {self.conversion[v] : [] for v in unique_words}\n",
    "        self.jump_factor = 0.85\n",
    "        self.threshold = 0.0001\n",
    "\n",
    "        for i in range(self.total_cnt):\n",
    "            token = filtered_tokens[i]\n",
    "            for j in range(N):\n",
    "                if i + j + 1 >= self.total_cnt:\n",
    "                    break\n",
    "                else:\n",
    "                    next_token = filtered_tokens[i + j + 1]\n",
    "                    if token[0] + N < next_token[0]:\n",
    "                        break\n",
    "                    else:\n",
    "                        idx = self.conversion[token[1]]\n",
    "                        next_idx = self.conversion[next_token[1]]\n",
    "                        if next_idx not in self.E[idx]:\n",
    "                            self.E[idx].append(next_idx)\n",
    "                            self.E[next_idx].append(idx)\n",
    "\n",
    "        self.conversion = {v : k for k, v in self.conversion.items()}\n",
    "        \n",
    "    def score_of(self, word):\n",
    "        neighbor_list = self.E[word]\n",
    "        temp = 0\n",
    "        for neighbor in neighbor_list:\n",
    "            temp += self.V[neighbor] / len(self.E[neighbor])\n",
    "        return (1 - self.jump_factor) + self.jump_factor * temp\n",
    "\n",
    "    def calculate_textrank(self):\n",
    "        flags = [False for i in range(self.unique_cnt)]\n",
    "        i = 0\n",
    "        iter_cnt = 0\n",
    "        while not all(flags):\n",
    "            prev_score = self.V[i]\n",
    "            curr_score = self.score_of(i)\n",
    "            self.V[i] = curr_score\n",
    "            if abs(prev_score - curr_score) < self.threshold:\n",
    "                flags[i] = True\n",
    "            i = (i + 1) % self.unique_cnt\n",
    "            if i == 0:\n",
    "                iter_cnt += 1\n",
    "        return iter_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = LexicalGraph(tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60 45 15\n"
     ]
    }
   ],
   "source": [
    "print(graph.total_cnt, graph.unique_cnt, graph.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "iter_cnt = graph.calculate_textrank()\n",
    "iter_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_sorted_scores = sorted(graph.V.items(), key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['deep', 'learning', 'models', 'domains', 'representation', 'natural', 'many', 'understanding', 'past', 'processing', 'text', 'results', 'designs', 'brief', 'description'] [2.4950098391377393, 1.8376872138634366, 1.7177995436033875, 1.6775278267469873, 1.4594681225663324, 1.3428682881571992, 1.3097628441072358, 1.2982581840332954, 1.2982562985335644, 1.2792770549162067, 1.1857486012418885, 1.075582623255793, 1.0741440967831228, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "potential_keywords = []\n",
    "potential_keywords_score = []\n",
    "cur_score = math.inf\n",
    "for i in range(graph.T):\n",
    "    cur_score = rev_sorted_scores[i][1]\n",
    "    word = graph.conversion[rev_sorted_scores[i][0]]\n",
    "    potential_keywords.append(word)\n",
    "    potential_keywords_score.append(cur_score)\n",
    "print(potential_keywords, potential_keywords_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_2 = ['summarization', 'information', 'abstractive', 'framework', 'element', 'items', 'coherent', 'previous', 'abstract', 'representation', 'source', 'documents']\n",
    "n_3 = ['framework', 'information', 'summarization', 'abstractive', 'summary', 'previous', 'text', 'abstract', 'representation', 'relies', 'element', 'coherent']\n",
    "n_4 = ['framework', 'summarization', 'information', 'summary', 'abstractive', 'representation', 'text', 'differs', 'previous', 'abstract', 'concept', 'coherent']\n",
    "n_5 = ['framework', 'summarization', 'information', 'abstract', 'abstractive', 'text', 'concept', 'models', 'differs', 'representation', 'relies', 'previous']\n",
    "n_6 = ['framework', 'summarization', 'information', 'text', 'abstractive', 'representation', 'relies', 'models', 'abstract', 'tac', 'concept', 'differs']\n",
    "n_7 = ['summarization', 'framework', 'text', 'information', 'abstractive', 'representation', 'abstract', 'relies', 'models', 'sentence', 'differs', 'tac']\n",
    "n_8 = ['summarization', 'framework', 'text', 'information', 'abstractive', 'representation', 'abstract', 'relies', 'models', 'sentence', 'differs', 'tac']\n",
    "n_9 = ['framework', 'summarization', 'text', 'representation', 'information', 'abstract', 'abstractive', 'sentence', 'previous', 'models', 'relies', 'concept']\n",
    "n_10 = ['framework', 'summarization', 'text', 'information', 'abstractive', 'abstract', 'representation', 'sentence', 'semantic', 'differs', 'analysis', 'previous'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_multi_word_keyword(potential_keywords, filtered_tokens):\n",
    "    relation = [t for t in filtered_tokens if t[1] in potential_keywords]\n",
    "    keywords = []\n",
    "    i = 0\n",
    "    while i < len(relation):\n",
    "        idx = relation[i][0]\n",
    "        keyword = relation[i][1]\n",
    "        flag = True\n",
    "        j = i + 1\n",
    "        while flag and j < len(relation):\n",
    "            next_idx = relation[j][0]\n",
    "            if next_idx == idx + 1:\n",
    "                keyword += ' ' + relation[j][1]\n",
    "                idx = next_idx\n",
    "                j += 1\n",
    "            else:\n",
    "                flag = False\n",
    "        i = j\n",
    "        if keyword not in keywords:\n",
    "            print (keyword)\n",
    "            keywords.append(keyword)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deep learning\nmany\nrepresentation\nresults\ndomains\ndesigns\nnatural\nprocessing\nbrief description\ndeep\nmany deep learning models\ntext\nmodels\nunderstanding\npast\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['deep learning',\n",
       " 'many',\n",
       " 'representation',\n",
       " 'results',\n",
       " 'domains',\n",
       " 'designs',\n",
       " 'natural',\n",
       " 'processing',\n",
       " 'brief description',\n",
       " 'deep',\n",
       " 'many deep learning models',\n",
       " 'text',\n",
       " 'models',\n",
       " 'understanding',\n",
       " 'past']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "final_keywords = combine_multi_word_keyword(potential_keywords, tokens)\n",
    "final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}